{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, explode, split\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from dependencies.elasticsearch import save_dataframes_to_elasticsearch\n",
    "from constant.constant import hadoop_namenode\n",
    "\n",
    "\n",
    "def pre_process_article_data(spark, data):\n",
    "    print('Start article')\n",
    "\n",
    "    # Đọc dữ liệu các bài báo đã có từ hadoop\n",
    "    schema = StructType([\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"guid\", StringType(), True),\n",
    "        StructField(\"link\", StringType(), True),\n",
    "        StructField(\"source\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"pubDate\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    articles = 0\n",
    "\n",
    "    try:\n",
    "        articles = (spark\n",
    "                    .read\n",
    "                    .format('parquet')\n",
    "                    .schema(schema)\n",
    "                    .load(hadoop_namenode + 'articles.parquet'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    data = data.na.fill(value='')\n",
    "\n",
    "    print('Read data success')\n",
    "\n",
    "    # Sử dụng spark map để tính time stamp và định danh duy nhất của từng bài báo\n",
    "    def get_article_id(row):\n",
    "        return ([\n",
    "            row.content,\n",
    "            row.description,\n",
    "            row.link,\n",
    "            row.source,\n",
    "            row.title,\n",
    "            (datetime.strptime(\n",
    "                row.pubDate[5:][:-6], '%d %b %Y %H:%M:%S') - timedelta(hours=7)).strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "            row.source + row.link.split('-')[-1].split('.')[0],\n",
    "        ])\n",
    "\n",
    "    new_article = data.rdd.map(get_article_id).toDF([\n",
    "        'content',\n",
    "        'description',\n",
    "        'link',\n",
    "        'source',\n",
    "        'title',\n",
    "        'pubDate',\n",
    "        'id',\n",
    "    ])\n",
    "\n",
    "    # Tìm các bài báo chưa được ghi trong hadoop tức là các bài báo mới để tránh trùng lắp\n",
    "    if articles != 0:\n",
    "\n",
    "        new_article = new_article.join(articles.withColumn('new', lit(False)).select(col('id'), col('new')), on=\"id\", how='left')\\\n",
    "            .withColumn('new', coalesce('new', lit(True)))\\\n",
    "            .filter(col('new') == True)\n",
    "\n",
    "    new_article.show()\n",
    "\n",
    "    new_article = new_article.select(col('id'), col('content'), col(\n",
    "        'description'), col('link'), col('source'), col('title'), col('pubDate'))\n",
    "\n",
    "    # Ghi lại các bài báo ra hadoop\n",
    "    (new_article\n",
    "        .write\n",
    "        .format('parquet')\n",
    "        .mode('append')\n",
    "        .save(hadoop_namenode + 'articles.parquet'))\n",
    "\n",
    "    print('Success append to articles.parquet')\n",
    "\n",
    "    return new_article"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
