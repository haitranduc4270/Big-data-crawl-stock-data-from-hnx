version: "3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    ports:
      - 9864:9864
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  pyspark-prj:
    image: jupyter/pyspark-notebook
    container_name: notebook
    ports:
      - "8888:8888"
    volumes:
      - ../big_data_20221/:/home/jovyan/work/

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-master-crawler:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master-crawler
    depends_on:
      - namenode
      - datanode
    ports:
      - "9080:8080"
      - "8077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-crawler:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-crawler
    depends_on:
      - spark-master
    ports:
      - "9082:9082"
    environment:
      - "SPARK_MASTER=spark://spark-master-crawler:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:7.15.1
  #   container_name: elasticsearch
  #   volumes:
  #     - esdata:/usr/share/elasticsearch/data
  #   ports:
  #     - 9200:9200
  #     - 9300:9300
  #   environment:
  #     - xpack.security.enabled=false
  #     - "discovery.type=single-node"

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:7.15.1
  #   container_name: kibana
  #   ports:
  #     - 5601:5601
  #   environment:
  #     ELASTICSEARCH_URL: http://elasticsearch:9200
  #     ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    expose:
      - "2181"
    ports:
      - 2181:2181

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    expose:
      - "9092"
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://localhost:19092
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://0.0.0.0:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
    depends_on:
      - zookeeper

  pyspark_application:
    image: myracoon/racoon_pyspark:latest
    container_name: pyspark_application
    restart: always
    depends_on:
      - namenode
      - datanode
      - spark-master
      - spark-worker
      # - elasticsearch
      # - kibana
      # - zookeeper
      # - kafka
    volumes:
      - ../big_data_20221/pyspark_application/:/app

  crawler:
    image: myracoon/racoon_pyspark:latest
    container_name: crawler
    restart: always
    depends_on:
      - namenode
      - datanode
      - spark-master
      - spark-worker
      # - elasticsearch
      # - kibana
      # - zookeeper
      # - kafka
    volumes:
      - ../big_data_20221/crawler/:/app

volumes:
  hadoop_namenode:
  hadoop_datanode: