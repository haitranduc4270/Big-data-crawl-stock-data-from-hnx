{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa4cd4e-e23f-4948-8ab0-12bc6ee12585",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'udfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mudfs\u001b[39;00m \u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mqueries\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mio_cluster\u001b[39;00m\n\u001b[1;32m     14\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     15\u001b[0m       StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m       StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMô tả công việc\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m       StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCách thức ứng tuyển\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m   ])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'udfs'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from operator import add\n",
    "import sys,os\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import udfs\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self,\n",
    "                elasticsearch_host,\n",
    "                elasticsearch_port,\n",
    "                elasticsearch_input_json,\n",
    "                elasticsearch_nodes_wan_only,\n",
    "                hdfs_namenode\n",
    "                 ):\n",
    "        self.elasticsearch_conf = {\n",
    "            'es.nodes': elasticsearch_host,\n",
    "            'es.port': elasticsearch_port,\n",
    "            \"es.input.json\":elasticsearch_input_json,\n",
    "            \"es.nodes.wan.only\": elasticsearch_nodes_wan_only\n",
    "        }\n",
    "        self.hdfs_namenode = hdfs_namenode\n",
    "        self.spark_app = None\n",
    "        \n",
    "    def get_elasticsearch_conf(self):\n",
    "        return self.elasticsearch_conf\n",
    "\n",
    "    def get_hdfs_namenode(self):\n",
    "        return self.hdfs_namenode\n",
    "\n",
    "    def initialize_spark_session(self,appName):\n",
    "        if self.spark_app == None :\n",
    "            self.spark_app = (SparkSession\n",
    "                        .builder.master(\"spark://spark-master:7077\")\n",
    "                        .appName(appName)\n",
    "                        .config(\"spark.jars\",\"/elasticsearch-hadoop-7.15.1.jar\")\n",
    "                        .config(\"spark.driver.extraClassPath\",\"/elasticsearch-hadoop-7.15.1.jar\")\n",
    "                        .config(\"spark.es.nodes\",self.elasticsearch_conf[\"es.nodes\"])\n",
    "                        .config(\"spark.es.port\",self.elasticsearch_conf[\"es.port\"])\n",
    "                        .config(\"spark.es.nodes.wan.only\",self.elasticsearch_conf[\"es.nodes.wan.only\"])\n",
    "                        .getOrCreate())\n",
    "        return self.spark_app\n",
    "\n",
    "def save_dataframes_to_hdfs(path,config,data_dfs,target_file_names):\n",
    "    \"\"\"\n",
    "        Function to store dataframe in hdfs\n",
    "        \n",
    "        Input:\n",
    "        \n",
    "        path: the directory path to store dataframe to\n",
    "        config: Config object\n",
    "        data_dfs: list of PySpark DataFrames to write\n",
    "        target_file_names: list of file names to store dataframes by        \n",
    "    \"\"\"\n",
    "\n",
    "    for data_df,target_file_name in zip(data_dfs,target_file_names):\n",
    "        print(\"Processing file: \",target_file_name)\n",
    "        print(\"Processing dataframe of type \",type(data_df))\n",
    "        data_df.write.on.mode(\"overwrite\").save(config.get_hdfs_namenode()+\"/\"+path+\"/\"+target_file_name)\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "      StructField(\"name\",StringType(),True),\n",
    "      StructField(\"Mô tả công việc\",StringType(),True),\n",
    "      StructField(\"Yêu cầu ứng viên\",StringType(),True),\n",
    "      StructField(\"Quyền lợi\",StringType(),True),\n",
    "      StructField(\"Cách thức ứng tuyển\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    APP_NAME=\"PreprocessData\"\n",
    "    \n",
    "    app_config = config.Config(elasticsearch_host=\"elasticsearch\",\n",
    "                               elasticsearch_port=\"9200\",\n",
    "                               elasticsearch_input_json=\"yes\",\n",
    "                               elasticsearch_nodes_wan_only=\"true\",\n",
    "                               hdfs_namenode=\"hdfs://namenode:9000\"\n",
    "                               )\n",
    "    spark = app_config.initialize_spark_session(APP_NAME)\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    raw_recruit_df = spark.read.schema(schema).option(\"multiline\",\"true\").json(\"hdfs://namenode:9000/data/rawdata/*.json\")\n",
    "    # raw_recruit_df.show(5)\n",
    "    extracted_recruit_df=raw_recruit_df.select(raw_recruit_df[\"name\"].alias(\"CompanyName\"),\n",
    "          udfs.extract_framework_plattform(\"Mô tả công việc\",\"Yêu cầu ứng viên\").alias(\"FrameworkPlattforms\"),\n",
    "          udfs.extract_language(\"Mô tả công việc\",\"Yêu cầu ứng viên\").alias(\"Languages\"),\n",
    "          udfs.extract_knowledge(\"Mô tả công việc\",\"Yêu cầu ứng viên\").alias(\"Knowledges\"),\n",
    "          udfs.normalize_salary(\"Quyền lợi\").alias(\"Salaries\")\n",
    "          )\n",
    "    extracted_recruit_df.cache()\n",
    "    # extracted_recruit_df.show(5)\n",
    "\n",
    "    ##========save extracted_recruit_df to hdfs========================\n",
    "    df_to_hdfs=(extracted_recruit_df,)\n",
    "    df_hdfs_name = (\"extracted_recruit.json\",)\n",
    "    save_dataframes_to_hdfs(\"data/extracteddata\", app_config, df_to_hdfs, df_hdfs_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
