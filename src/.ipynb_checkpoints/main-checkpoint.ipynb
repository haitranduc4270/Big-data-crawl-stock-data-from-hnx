{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12fdb75-4cbe-4af2-9d8c-8da90c1400ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2770692f-2d74-4462-8b69-6a3ca8b93df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, concat, lit, from_json, schema_of_json, flatten, json_tuple\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2687735-0174-4ac9-8bd0-b4d7a0e13cb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o110.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (c2e8ac932004 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1690, in verify_struct\n    verifier(obj.get(f))\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field name: DoubleType() can not accept object '0.2' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1690, in verify_struct\n    verifier(obj.get(f))\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field name: DoubleType() can not accept object '0.2' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rdd, schema)\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mna\u001b[38;5;241m.\u001b[39mfill(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, subset\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstname\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 21\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o110.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (c2e8ac932004 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1690, in verify_struct\n    verifier(obj.get(f))\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field name: DoubleType() can not accept object '0.2' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 910, in prepare\n    verify_func(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1690, in verify_struct\n    verifier(obj.get(f))\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1722, in verify\n    verify_value(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1716, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 1592, in verify_acceptable_types\n    raise TypeError(\nTypeError: field name: DoubleType() can not accept object '0.2' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data = [{\"stock_code\": 'AAA', 'name': '0.2'},\n",
    "       {\"stock_code\": 'AAT', 'name': '0.2'},\n",
    "       {\"stock_code\": 'AAS', 'name': '0.3'}]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = spark.createDataFrame(rdd)\n",
    "\n",
    "# df = df.na.fill(value='', subset=(['firstname']))\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94f18306-d629-47f8-9c10-d2c4419f27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----+----+----+---+\n",
      "|name|stock_code|   f|   c|   d|  e|\n",
      "+----+----------+----+----+----+---+\n",
      "| aaa|       AAA|null|null|null|  0|\n",
      "| aat|       AAT|null|null|null|  0|\n",
      "| aas|       AAS|null|null|null|  0|\n",
      "+----+----------+----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# def detect_data(column, df, data_type):\n",
    "#       if not column in df.columns:\n",
    "#         ret = lit(0).cast(data_type)\n",
    "#       else:\n",
    "#         ret = col(column).cast(data_type)\n",
    "\n",
    "#       return ret\n",
    "\n",
    "# df = df.withColumn('e', detect_data('g', df, LongType()))\n",
    "# df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75374fd2-2303-429c-a4b1-27530adf6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- best1Bid: long (nullable = true)\n",
      " |-- best1BidVol: long (nullable = true)\n",
      " |-- best1Offer: long (nullable = true)\n",
      " |-- best1OfferVol: long (nullable = true)\n",
      " |-- best2Bid: long (nullable = true)\n",
      " |-- best2BidVol: long (nullable = true)\n",
      " |-- best2Offer: long (nullable = true)\n",
      " |-- best2OfferVol: long (nullable = true)\n",
      " |-- best3Bid: long (nullable = true)\n",
      " |-- best3BidVol: long (nullable = true)\n",
      " |-- best3Offer: long (nullable = true)\n",
      " |-- best3OfferVol: long (nullable = true)\n",
      " |-- ceiling: long (nullable = true)\n",
      " |-- floor: long (nullable = true)\n",
      " |-- highest: long (nullable = true)\n",
      " |-- lowest: long (nullable = true)\n",
      " |-- matchedPrice: long (nullable = true)\n",
      " |-- matchedVolume: long (nullable = true)\n",
      " |-- nmTotalTradedQty: long (nullable = true)\n",
      " |-- priceChange: string (nullable = true)\n",
      " |-- priceChangePercent: string (nullable = true)\n",
      " |-- refPrice: long (nullable = true)\n",
      " |-- stock_code: string (nullable = true)\n",
      " |-- stock_exchange: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n",
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+----------------+\n",
      "|best1Bid|best1BidVol|best1Offer|best1OfferVol|best2Bid|best2BidVol|best2Offer|best2OfferVol|best3Bid|best3BidVol|best3Offer|best3OfferVol|ceiling|floor |highest|lowest|matchedPrice|matchedVolume|nmTotalTradedQty|priceChange|priceChangePercent|refPrice|stock_code|stock_exchange|time_stamp      |\n",
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+----------------+\n",
      "|132500  |13300      |133000    |1800         |132000  |26500      |133500    |5900         |131500  |13700      |134000    |88000        |141500 |123500|134500 |132000|133000      |1760         |346400          |50.00      |0.4               |132500  |NBB       |hose          |2023-01-19T07:54|\n",
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checker = spark.read.json(\"hdfs://namenode:9000/project20221-real/clean/stock_price_realtime/hose/01-19-2023/01-19-2023-07-54-48.json\")\n",
    "checker.printSchema()\n",
    "checker.show(1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5c08c8-3af7-4b15-8c0d-dcc44cdc4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0.7093728620928866\n",
      "heloooo\n",
      "1.3366998220214015\n",
      "heloooo\n",
      "0.7925297386604591\n",
      "heloooo\n",
      "1.0051160542497017\n",
      "heloooo\n",
      "1.2364992939190764\n",
      "heloooo\n",
      "0.5808608580788309\n",
      "heloooo\n",
      "0.6558644614041969\n",
      "heloooo\n",
      "0.8605907921556382\n",
      "heloooo\n",
      "1.4908527301889292\n",
      "heloooo\n",
      "0.8622193830845466\n",
      "heloooo\n",
      "1.4667794291400649\n",
      "heloooo\n",
      "1.2842022464929372\n",
      "heloooo\n",
      "1.0787284777839483\n",
      "heloooo\n",
      "1.1523283222795397\n",
      "heloooo\n",
      "1.307082259556025\n",
      "heloooo\n",
      "1.0445507098070816\n",
      "heloooo\n",
      "1.0021279864664736\n",
      "heloooo\n",
      "1.3540770652956657\n",
      "heloooo\n",
      "0.5512985201488609\n",
      "heloooo\n",
      "0.9559428045400173\n",
      "heloooo\n",
      "1.3455983070291748\n",
      "heloooo\n",
      "1.405635684878358\n",
      "heloooo\n",
      "0.7638542413704508\n",
      "heloooo\n",
      "0.6405009332974826\n",
      "heloooo\n",
      "0.7458063357097328\n",
      "heloooo\n",
      "0.7813603690929636\n",
      "heloooo\n",
      "1.0214861597650202\n",
      "heloooo\n",
      "0.9589203416279545\n",
      "heloooo\n",
      "0.5160617004123231\n",
      "heloooo\n",
      "1.3143352830615136\n",
      "heloooo\n",
      "1.2487813947419015\n",
      "heloooo\n",
      "0.6180038126747036\n",
      "heloooo\n",
      "1.3266785300544255\n",
      "heloooo\n",
      "1.0039609263194655\n",
      "heloooo\n",
      "1.3164954389457377\n",
      "heloooo\n",
      "0.6081943225667739\n",
      "heloooo\n",
      "1.227848253570019\n",
      "heloooo\n",
      "0.5451105780292235\n",
      "heloooo\n",
      "1.2081744126638438\n",
      "heloooo\n",
      "0.7553713918655132\n",
      "heloooo\n",
      "0.5537226082542771\n",
      "heloooo\n",
      "0.573548702298057\n",
      "heloooo\n",
      "0.786575124006302\n",
      "heloooo\n",
      "0.7101233802875092\n",
      "heloooo\n",
      "0.5342291310835083\n",
      "heloooo\n",
      "1.4303293206402703\n",
      "heloooo\n",
      "1.0796790411209107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "print('started')\n",
    "def start_crawler ():\n",
    "    while True:\n",
    "        sleep_time = random.uniform(0.5,1.5)\n",
    "        print(sleep_time)\n",
    "        time.sleep(sleep_time)\n",
    "        print(\"heloooo\")\n",
    "        \n",
    "start_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3fd07-0239-4c94-a2df-105666d2e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "print('start')\n",
    "\n",
    "spark_sess = (\\\n",
    "    SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[1]')\\\n",
    "    .appName('pyspark_app')\\\n",
    "    .getOrCreate())\n",
    "\n",
    "print('start')\n",
    "\n",
    "csv = spark_sess.read.option(\"inferSchema\", True).option(\"header\",True).csv(\"../crawler/data/ssiDataApi.csv\")\n",
    "csv.printSchema()\n",
    "csv.show(1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fd12c6-431c-49f4-a470-ec559563503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+-------------------+\n",
      "|best1Bid|best1BidVol|best1Offer|best1OfferVol|best2Bid|best2BidVol|best2Offer|best2OfferVol|best3Bid|best3BidVol|best3Offer|best3OfferVol|ceiling| floor|highest|lowest|matchedPrice|matchedVolume|nmTotalTradedQty|priceChange|priceChangePercent|refPrice|stock_code|stock_exchange|         time_stamp|\n",
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+-------------------+\n",
      "|  220000|        200|    245000|          100|  216000|        500|    246000|          500|  215000|        900|         0|            0| 247000|203000| 244000|244000|           0|            0|             100|           |                  |  225000|       PMS|           hnx|2023-01-18T07:30:18|\n",
      "|       0|          0|         0|          500|       0|          0|    255000|          700|       0|          0|    283000|         2000| 311000|255000|      0|     0|      283000|            0|               0|       0.00|               0.0|  283000|       SDU|           hnx|2023-01-18T07:30:18|\n",
      "|       0|          0|    192000|         1000|       0|          0|    210000|          100|       0|          0|    234000|          100| 234000|192000|      0|     0|           0|            0|               0|           |                  |  213000|       SDG|           hnx|2023-01-18T07:30:18|\n",
      "|       0|          0|    639000|         3800|       0|          0|    650000|          200|       0|          0|    709000|         1400| 779000|639000|      0|     0|           0|            0|               0|           |                  |  709000|       SGH|           hnx|2023-01-18T07:30:18|\n",
      "|  159000|       8500|    160000|        92300|  158000|      23900|    161000|        71500|  157000|      22100|    162000|        68400| 173000|143000| 161000|157000|           0|            0|          432800|           |                  |  158000|       NAG|           hnx|2023-01-18T07:30:18|\n",
      "|   46000|        100|     47000|         1200|   45000|        200|     48000|         3200|   43000|       6000|     49000|          700|  51000| 43000|  47000| 47000|           0|            0|             100|           |                  |   47000|       PCT|           hnx|2023-01-18T07:30:18|\n",
      "|  124000|        200|    127000|         1400|  122000|       1500|    128000|         4300|  121000|       1000|    129000|         2500| 136000|112000| 130000|124000|           0|            0|            5700|           |                  |  124000|       PBP|           hnx|2023-01-18T07:30:18|\n",
      "|       0|       4000|     21000|        62000|   20000|      23700|     22000|        98200|   19000|      71900|         0|            0|  22000| 18000|  21000| 20000|       21000|         4000|           13700|     100.00|               5.0|   20000|       PV2|           hnx|2023-01-18T07:30:18|\n",
      "|   75000|       8000|         0|          800|   74000|      20200|     76000|         4000|   73000|       4800|     77000|        41300|  84000| 70000|  79000| 70000|       75000|          800|           84500|    -200.00|              -2.6|   77000|       OCH|           hnx|2023-01-18T07:30:18|\n",
      "|   66000|        100|         0|         3000|   65000|       2000|     68000|         8700|   64000|       1400|     69000|         5100|  74000| 62000|  68000| 62000|       64000|         3000|           95700|    -400.00|              -5.9|   68000|       SDA|           hnx|2023-01-18T07:30:18|\n",
      "|   74000|        300|     79000|          100|   72000|       2000|         0|            0|   67000|        300|         0|            0|  79000| 65000|      0|     0|           0|            0|               0|           |                  |   72000|       PEN|           hnx|2023-01-18T07:30:18|\n",
      "|  168000|       2000|    198000|          100|  165000|        200|         0|            0|  163000|       3000|         0|            0| 198000|162000| 162000|162000|           0|            0|             100|           |                  |  180000|       NBW|           hnx|2023-01-18T07:30:18|\n",
      "|  135000|        200|    148000|          100|  134000|        200|    149000|         1000|       0|          0|    150000|         1000| 162000|134000| 147000|138000|           0|            0|             600|           |                  |  148000|       PIC|           hnx|2023-01-18T07:30:18|\n",
      "|   57000|        100|     58000|         1600|   56000|       3000|     59000|          200|   55000|       6900|     60000|         2800|  62000| 52000|  57000| 56000|           0|            0|            8100|           |                  |   57000|       NDX|           hnx|2023-01-18T07:30:18|\n",
      "|  339000|       8100|    340000|          200|  338000|        100|    342000|         2500|  337000|       3800|    343000|         1000| 374000|306000| 340000|334000|           0|            0|           35200|           |                  |  340000|       NTP|           hnx|2023-01-18T07:30:18|\n",
      "|   42000|      32600|     43000|       223100|   41000|     348000|     44000|       297200|   40000|     143800|     45000|       421900|  45000| 37000|  43000| 41000|       42000|            0|          527500|     100.00|               2.4|   41000|       MST|           hnx|2023-01-18T07:30:18|\n",
      "|  124000|       1900|    136000|          300|  123000|        100|    138000|          200|       0|          0|    143000|         3600| 147000|121000|      0|     0|           0|            0|               0|           |                  |  134000|       SMN|           hnx|2023-01-18T07:30:18|\n",
      "|       0|       null|         0|         null|       0|       null|         0|         null|       0|       null|         0|         null|      0|     0|      0|     0|           0|            0|               0|           |                  |  549260| NVL122001|           hnx|2023-01-18T07:30:18|\n",
      "|  440000|        600|         0|            0|  432000|        500|         0|            0|  428000|        500|         0|            0| 459000|377000| 459000|430000|           0|            0|            6500|           |                  |  418000|       PRC|           hnx|2023-01-18T07:30:18|\n",
      "|   72000|      10100|     73000|          400|   71000|       5200|     74000|        29100|   70000|      14700|     75000|        40200|  80000| 66000|  74000| 73000|           0|            0|           42000|           |                  |   73000|       PVG|           hnx|2023-01-18T07:30:18|\n",
      "+--------+-----------+----------+-------------+--------+-----------+----------+-------------+--------+-----------+----------+-------------+-------+------+-------+------+------------+-------------+----------------+-----------+------------------+--------+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs = spark.read.json('hdfs://namenode:9000/project20221/clean/stock_price_realtime/hnx/01-18-2023/01-18-2023-07-30-18.json')\n",
    "cs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9bb1587-7856-4ec3-ad0d-cec4b6d65eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 04:32:00\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "elasticsearch_time_format = '%Y-%m-%dT%H:%M:%S'\n",
    "\n",
    "\n",
    "def is_in_exchange_time():\n",
    "    now = datetime.strptime('2023-01-24T04:32:00', '%Y-%m-%dT%H:%M:%S')\n",
    "    print(now)\n",
    "    # now = datetime.now() + timedelta(hours=7)\n",
    "    today = now.strftime('%Y-%m-%d')\n",
    "\n",
    "    start_mor = datetime.strptime(\n",
    "        today + 'T' + '09:00:00', elasticsearch_time_format)\n",
    "    end_mor = datetime.strptime(\n",
    "        today + 'T' + '11:30:00', elasticsearch_time_format)\n",
    "    start_eve = datetime.strptime(\n",
    "        today + 'T' + '13:00:00', elasticsearch_time_format)\n",
    "    end_eve = datetime.strptime(\n",
    "        today + 'T' + '15:00:00', elasticsearch_time_format)\n",
    "\n",
    "    if (now > start_mor and now < end_mor) or (now > start_eve and now < end_eve):\n",
    "        return 1\n",
    "    elif now > end_eve:\n",
    "        return 2\n",
    "    elif now < start_mor:\n",
    "        return 3\n",
    "    elif now > end_mor and now < start_eve:\n",
    "        return 4\n",
    "    \n",
    "\n",
    "print(is_in_exchange_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f08247-8139-44d0-a374-add7cfb64cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_stock_info ():\n",
    "    result = requests.post('https://wgateway-iboard.ssi.com.vn/graphql', json={\n",
    "        \"operationName\" : \"stockRealtimesByGroup\",\n",
    "        \"query\": \"query stockRealtimes($exchange: String) {\\n  stockRealtimes(exchange: $exchange) {\\n    stockNo\\n    ceiling\\n    floor\\n    refPrice\\n    stockSymbol\\n    stockType\\n    exchange\\n    matchedPrice\\n    matchedVolume\\n    priceChange\\n    priceChangePercent\\n    highest\\n    avgPrice\\n    lowest\\n    nmTotalTradedQty\\n    best1Bid\\n    best1BidVol\\n    best2Bid\\n    best2BidVol\\n    best3Bid\\n    best3BidVol\\n    best4Bid\\n    best4BidVol\\n    best5Bid\\n    best5BidVol\\n    best6Bid\\n    best6BidVol\\n    best7Bid\\n    best7BidVol\\n    best8Bid\\n    best8BidVol\\n    best9Bid\\n    best9BidVol\\n    best10Bid\\n    best10BidVol\\n    best1Offer\\n    best1OfferVol\\n    best2Offer\\n    best2OfferVol\\n    best3Offer\\n    best3OfferVol\\n    best4Offer\\n    best4OfferVol\\n    best5Offer\\n    best5OfferVol\\n    best6Offer\\n    best6OfferVol\\n    best7Offer\\n    best7OfferVol\\n    best8Offer\\n    best8OfferVol\\n    best9Offer\\n    best9OfferVol\\n    best10Offer\\n    best10OfferVol\\n    buyForeignQtty\\n    buyForeignValue\\n    sellForeignQtty\\n    sellForeignValue\\n    caStatus\\n    tradingStatus\\n    remainForeignQtty\\n    currentBidQty\\n    currentOfferQty\\n    session\\n    __typename\\n  }\\n}\\n\"\n",
    "    ,\n",
    "        \"variables\": {\n",
    "            \"exchange\": \"hnx\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    hnx = json.loads(result.text)['data']['stockRealtimes']\n",
    "    print(len(hnx))\n",
    "    \n",
    "    result = requests.post('https://wgateway-iboard.ssi.com.vn/graphql', json={\n",
    "        \"operationName\" : \"stockRealtimesByGroup\",\n",
    "        \"query\": \"query stockRealtimes($exchange: String) {\\n  stockRealtimes(exchange: $exchange) {\\n    stockNo\\n    ceiling\\n    floor\\n    refPrice\\n    stockSymbol\\n    stockType\\n    exchange\\n    matchedPrice\\n    matchedVolume\\n    priceChange\\n    priceChangePercent\\n    highest\\n    avgPrice\\n    lowest\\n    nmTotalTradedQty\\n    best1Bid\\n    best1BidVol\\n    best2Bid\\n    best2BidVol\\n    best3Bid\\n    best3BidVol\\n    best4Bid\\n    best4BidVol\\n    best5Bid\\n    best5BidVol\\n    best6Bid\\n    best6BidVol\\n    best7Bid\\n    best7BidVol\\n    best8Bid\\n    best8BidVol\\n    best9Bid\\n    best9BidVol\\n    best10Bid\\n    best10BidVol\\n    best1Offer\\n    best1OfferVol\\n    best2Offer\\n    best2OfferVol\\n    best3Offer\\n    best3OfferVol\\n    best4Offer\\n    best4OfferVol\\n    best5Offer\\n    best5OfferVol\\n    best6Offer\\n    best6OfferVol\\n    best7Offer\\n    best7OfferVol\\n    best8Offer\\n    best8OfferVol\\n    best9Offer\\n    best9OfferVol\\n    best10Offer\\n    best10OfferVol\\n    buyForeignQtty\\n    buyForeignValue\\n    sellForeignQtty\\n    sellForeignValue\\n    caStatus\\n    tradingStatus\\n    remainForeignQtty\\n    currentBidQty\\n    currentOfferQty\\n    session\\n    __typename\\n  }\\n}\\n\"\n",
    "    ,\n",
    "        \"variables\": {\n",
    "            \"exchange\": \"hose\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    hose = json.loads(result.text)['data']['stockRealtimes']\n",
    "    print(len(hose))\n",
    "    \n",
    "    for stock in hnx:\n",
    "        info = requests.post('https://finfo-iboard.ssi.com.vn/graphql', json={\n",
    "           \"operationName\":\"companyProfile\",\n",
    "           \"variables\":{\n",
    "              \"symbol\": stock['stockSymbol'],\n",
    "              \"language\":\"vn\"\n",
    "           },\n",
    "           \"query\":\"query companyProfile($symbol: String!, $language: String) {\\n  companyProfile(symbol: $symbol, language: $language) {\\n    symbol\\n    subsectorcode\\n    industryname\\n    supersector\\n    sector\\n    subsector\\n    foundingdate\\n    chartercapital\\n    numberofemployee\\n    banknumberofbranch\\n    companyprofile\\n    listingdate\\n    exchange\\n    firstprice\\n    issueshare\\n    listedvalue\\n    companyname\\n    __typename\\n  }\\n  companyStatistics(symbol: $symbol) {\\n    symbol\\n    ttmtype\\n    marketcap\\n    sharesoutstanding\\n    bv\\n    beta\\n    eps\\n    dilutedeps\\n    pe\\n    pb\\n    dividendyield\\n    totalrevenue\\n    profit\\n    asset\\n    roe\\n    roa\\n    npl\\n    financialleverage\\n    __typename\\n  }\\n}\\n\"\n",
    "        })\n",
    "        print(json.loads(info.text))\n",
    "        \n",
    "        \n",
    "    for stock in hose:\n",
    "        info = requests.post('https://finfo-iboard.ssi.com.vn/graphql', json={\n",
    "           \"operationName\":\"companyProfile\",\n",
    "           \"variables\":{\n",
    "              \"symbol\": stock['stockSymbol'],\n",
    "              \"language\":\"vn\"\n",
    "           },\n",
    "           \"query\":\"query companyProfile($symbol: String!, $language: String) {\\n  companyProfile(symbol: $symbol, language: $language) {\\n    symbol\\n    subsectorcode\\n    industryname\\n    supersector\\n    sector\\n    subsector\\n    foundingdate\\n    chartercapital\\n    numberofemployee\\n    banknumberofbranch\\n    companyprofile\\n    listingdate\\n    exchange\\n    firstprice\\n    issueshare\\n    listedvalue\\n    companyname\\n    __typename\\n  }\\n  companyStatistics(symbol: $symbol) {\\n    symbol\\n    ttmtype\\n    marketcap\\n    sharesoutstanding\\n    bv\\n    beta\\n    eps\\n    dilutedeps\\n    pe\\n    pb\\n    dividendyield\\n    totalrevenue\\n    profit\\n    asset\\n    roe\\n    roa\\n    npl\\n    financialleverage\\n    __typename\\n  }\\n}\\n\"\n",
    "        })\n",
    "        print(json.loads(info.text))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "get_all_stock_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770151a4-bc2a-4ad2-ae50-5a9c188911c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "from constant.constant import hadoop_namenode, time_format, date_format, time_format, elasticsearch_time_format, elasticsearch_index\n",
    "from services.udfs.ssi import price_ssi, volume_ssi\n",
    "from dependencies.elasticsearch import save_dataframes_to_elasticsearch\n",
    "\n",
    "\n",
    "def process_ssi_stock_data(spark, data, config, time_stamp):\n",
    "    try:\n",
    "        print('Start')\n",
    "        print(datetime.now())\n",
    "        time_stamp = datetime.strptime(time_stamp, '%m-%d-%Y-%H-%M-%S')\n",
    "\n",
    "        data = data\\\n",
    "            .distinct()\\\n",
    "            .withColumn('best1Bid', price_ssi(col('best1Bid')))\\\n",
    "            .withColumn('best2Bid', price_ssi(col('best2Bid')))\\\n",
    "            .withColumn('best3Bid', price_ssi(col('best3Bid')))\\\n",
    "            .withColumn('best1Offer', price_ssi(col('best1Offer')))\\\n",
    "            .withColumn('best2Offer', price_ssi(col('best2Offer')))\\\n",
    "            .withColumn('best3Offer', price_ssi(col('best3Offer')))\\\n",
    "            .withColumn('lowest', price_ssi(col('lowest')))\\\n",
    "            .withColumn('highest', price_ssi(col('highest')))\\\n",
    "            .withColumn('refPrice', price_ssi(col('refPrice')))\\\n",
    "            .withColumn('floor', price_ssi(col('floor')))\\\n",
    "            .withColumn('ceiling', price_ssi(col('ceiling')))\\\n",
    "            .withColumn('matchedPrice', price_ssi(col('matchedPrice')))\\\n",
    "            .withColumn('time_stamp', lit(time_stamp.strftime(elasticsearch_time_format)))\\\n",
    "            .withColumnRenamed('exchange', 'stock_exchange')\\\n",
    "            .withColumnRenamed('stockSymbol', 'stock_code')\n",
    "\n",
    "        stock_info_clean = data.select(\n",
    "            col('time_stamp'),\n",
    "            col('stock_code'),\n",
    "            col('stock_exchange'),\n",
    "            col('best1Bid'),\n",
    "            col('best1BidVol'),\n",
    "            col('best2Bid'),\n",
    "            col('best2BidVol'),\n",
    "            col('best3Bid'),\n",
    "            col('best3BidVol'),\n",
    "            col('best1Offer'),\n",
    "            col('best1OfferVol'),\n",
    "            col('best2Offer'),\n",
    "            col('best2OfferVol'),\n",
    "            col('best3Offer'),\n",
    "            col('best3OfferVol'),\n",
    "            col('priceChange'),\n",
    "            col('priceChangePercent'),\n",
    "            col('nmTotalTradedQty'),\n",
    "            col('lowest'),\n",
    "            col('highest'),\n",
    "            col('matchedPrice'),\n",
    "            col('matchedVolume'),\n",
    "            col('refPrice'),\n",
    "            col('floor'),\n",
    "            col('ceiling'))\n",
    "\n",
    "        data_dir = hadoop_namenode + config['hadoop_clean_dir'] + \\\n",
    "            config['source']['body']['variables']['exchange'] + '/' + \\\n",
    "            time_stamp.strftime(date_format) + '/' + \\\n",
    "            time_stamp.strftime(time_format) + '.json'\n",
    "\n",
    "        (stock_info_clean\n",
    "            .write\n",
    "            .format('json')\n",
    "            .mode('overwrite')\n",
    "            .save(data_dir))\n",
    "\n",
    "        save_dataframes_to_elasticsearch(stock_info_clean, elasticsearch_index, {\n",
    "            'es.nodes': 'elasticsearch',\n",
    "            'es.port': '9200',\n",
    "            \"es.input.json\": 'yes',\n",
    "            \"es.nodes.wan.only\": 'true'\n",
    "        })\n",
    "\n",
    "        print('Success save to ' + data_dir)\n",
    "\n",
    "        print('End')\n",
    "        print(datetime.now())\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1248b4f7-aadb-4f41-9632-50dcc6b2da47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, concat, lit, from_json, schema_of_json, flatten, json_tuple\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "with open('../pyspark_application/data/stock.json', 'r') as openfile:\n",
    "                stock_info = json.load(openfile)\n",
    "stock_info = stock_info['data']\n",
    "\n",
    "# json = spark.read.json('../backup_data/19_1_data.json')\n",
    "# b = json.write.format(\"json\").mode(\"overwrite\").save('hdfs://namenode:9000/project20221-data/19-1/a.json')\n",
    "\n",
    "c = spark.read.option(\"multiLine\", \"true\").json(\n",
    "    'hdfs://namenode:9000/project20221-data/19-1/a.json')\n",
    "\n",
    "print(c.count())\n",
    "\n",
    "# a = json.select(col('_source'))\n",
    "\n",
    "\n",
    "# stock_info_clean = a\\\n",
    "#     .withColumn('time_stamp', col('_source.time_stamp'))\\\n",
    "#     .withColumn('stock_code', col('_source.stock_code'))\\\n",
    "#     .withColumn('stock_exchange', col('_source.stock_exchange'))\\\n",
    "#     .withColumn('best1Bid', col('_source.best1Bid'))\\\n",
    "#     .withColumn('best1BidVol', col('_source.best1BidVol'))\\\n",
    "#     .withColumn('best2Bid', col('_source.best2Bid'))\\\n",
    "#     .withColumn('best2BidVol', col('_source.best2BidVol'))\\\n",
    "#     .withColumn('best3Bid', col('_source.best3Bid'))\\\n",
    "#     .withColumn('best3BidVol', col('_source.best3BidVol'))\\\n",
    "#     .withColumn('best1Offer', col('_source.best1Offer'))\\\n",
    "#     .withColumn('best1OfferVol', col('_source.best1OfferVol'))\\\n",
    "#     .withColumn('best2Offer', col('_source.best2Offer'))\\\n",
    "#     .withColumn('best2OfferVol', col('_source.best2OfferVol'))\\\n",
    "#     .withColumn('best3Offer', col('_source.best3Offer'))\\\n",
    "#     .withColumn('best3OfferVol', col('_source.best3OfferVol'))\\\n",
    "#     .withColumn('priceChange', col('_source.priceChange'))\\\n",
    "#     .withColumn('priceChangePercent', col('_source.priceChangePercent'))\\\n",
    "#     .withColumn('nmTotalTradedQty', col('_source.nmTotalTradedQty'))\\\n",
    "#     .withColumn('lowest', col('_source.lowest'))\\\n",
    "#     .withColumn('highest', col('_source.highest'))\\\n",
    "#     .withColumn('matchedPrice', col('_source.matchedPrice'))\\\n",
    "#     .withColumn('matchedVolume', col('_source.matchedVolume'))\\\n",
    "#     .withColumn('refPrice', col('_source.refPrice'))\\\n",
    "#     .withColumn('floor', col('_source.floor'))\\\n",
    "#     .withColumn('ceiling', col('_source.ceiling'))\n",
    "\n",
    "# stock_info_clean = stock_info_clean.select(\n",
    "#     col('time_stamp'),\n",
    "#     col('stock_code'),\n",
    "#     col('stock_exchange'),\n",
    "#     col('best1Bid'),\n",
    "#     col('best1BidVol'),\n",
    "#     col('best2Bid'),\n",
    "#     col('best2BidVol'),\n",
    "#     col('best3Bid'),\n",
    "#     col('best3BidVol'),\n",
    "#     col('best1Offer'),\n",
    "#     col('best1OfferVol'),\n",
    "#     col('best2Offer'),\n",
    "#     col('best2OfferVol'),\n",
    "#     col('best3Offer'),\n",
    "#     col('best3OfferVol'),\n",
    "#     col('priceChange'),\n",
    "#     col('priceChangePercent'),\n",
    "#     col('nmTotalTradedQty'),\n",
    "#     col('lowest'),\n",
    "#     col('highest'),\n",
    "#     col('matchedPrice'),\n",
    "#     col('matchedVolume'),\n",
    "#     col('refPrice'),\n",
    "#     col('floor'),\n",
    "#     col('ceiling'))\n",
    "\n",
    "# def price_ssi(price):\n",
    "#     if price is None:\n",
    "#         return 0\n",
    "#     return price * 1\n",
    "\n",
    "# def volume_ssi(volume):\n",
    "#     if volume is None:\n",
    "#         return 0\n",
    "#     return volume * 1\n",
    "\n",
    "# def to_number(string):\n",
    "#     if string is None:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         try:\n",
    "#             number = round(float(string), 4)\n",
    "#             return number\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(string)\n",
    "\n",
    "# def reformat(row):\n",
    "#     index = row.stock_code + '-' + row.stock_exchange\n",
    "#     return ([\n",
    "#         row.stock_code,\n",
    "#         row.stock_exchange,\n",
    "#         row.priceChange,\n",
    "#         row.priceChangePercent,\n",
    "#         row.nmTotalTradedQty,\n",
    "#         price_ssi(row.best1Bid),\n",
    "#         price_ssi(row.best2Bid),\n",
    "#         price_ssi(row.best3Bid),\n",
    "#         price_ssi(row.best1Offer),\n",
    "#         price_ssi(row.best2Offer),\n",
    "#         price_ssi(row.best3Offer),\n",
    "#         price_ssi(row.lowest),\n",
    "#         price_ssi(row.highest),\n",
    "#         price_ssi(row.refPrice),\n",
    "#         price_ssi(row.floor),\n",
    "#         price_ssi(row.ceiling),\n",
    "#         price_ssi(row.matchedPrice),\n",
    "#         volume_ssi(row.best1BidVol),\n",
    "#         volume_ssi(row.best2BidVol),\n",
    "#         volume_ssi(row.best3BidVol),\n",
    "#         volume_ssi(row.best1OfferVol),\n",
    "#         volume_ssi(row.best2OfferVol),\n",
    "#         volume_ssi(row.best3OfferVol),\n",
    "#         volume_ssi(row.matchedVolume),\n",
    "#         stock_info[index]['companyProfile'][\"subsectorcode\"],\n",
    "#         stock_info[index]['companyProfile'][\"industryname\"],\n",
    "#         stock_info[index]['companyProfile'][\"supersector\"],\n",
    "#         stock_info[index]['companyProfile'][\"sector\"],\n",
    "#         stock_info[index]['companyProfile'][\"subsector\"],\n",
    "#         stock_info[index]['companyProfile'][\"chartercapital\"],\n",
    "#         stock_info[index]['companyProfile'][\"numberofemployee\"],\n",
    "#         stock_info[index]['companyProfile'][\"issueshare\"],\n",
    "#         stock_info[index]['companyProfile'][\"firstprice\"],\n",
    "#         stock_info[index]['companyStatistics'][\"sharesoutstanding\"],\n",
    "#         stock_info[index]['companyStatistics'][\"marketcap\"],\n",
    "#     ])\n",
    "\n",
    "# stock_info_clean = stock_info_clean.rdd.map(reformat).toDF([\n",
    "#     'stock_code',\n",
    "#     'stock_exchange',\n",
    "#     'priceChange',\n",
    "#     'priceChangePercent',\n",
    "#     'nmTotalTradedQty',\n",
    "#     'best1Bid',\n",
    "#     'best2Bid',\n",
    "#     'best3Bid',\n",
    "#     'best1Offer',\n",
    "#     'best2Offer',\n",
    "#     'best3Offer',\n",
    "#     'lowest',\n",
    "#     'highest',\n",
    "#     'refPrice',\n",
    "#     'floor',\n",
    "#     'ceiling',\n",
    "#     'matchedPrice',\n",
    "#     'best1BidVol',\n",
    "#     'best2BidVol',\n",
    "#     'best3BidVol',\n",
    "#     'best1OfferVol',\n",
    "#     'best2OfferVol',\n",
    "#     'best3OfferVol',\n",
    "#     'matchedVolume',\n",
    "#     'subsectorcode',\n",
    "#     'industryname',\n",
    "#     'supersector',\n",
    "#     'sector',\n",
    "#     'subsector',\n",
    "#     'chartercapital',\n",
    "#     'numberofemployee',\n",
    "#     'issueshare',\n",
    "#     'firstprice',\n",
    "#     'sharesoutstanding',\n",
    "#     'marketcap',\n",
    "# ])\n",
    "\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"priceChange\",stock_info_clean.priceChange.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"priceChangePercent\",stock_info_clean.priceChangePercent.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"chartercapital\",stock_info_clean.chartercapital.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"numberofemployee\",stock_info_clean.numberofemployee.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"issueshare\",stock_info_clean.issueshare.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"firstprice\",stock_info_clean.firstprice.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"sharesoutstanding\",stock_info_clean.sharesoutstanding.cast(DoubleType()))\n",
    "# stock_info_clean = stock_info_clean.withColumn(\"marketcap\",stock_info_clean.marketcap.cast(DoubleType()))\n",
    "\n",
    "# stock_info_clean.printSchema()\n",
    "# stock_info_clean.show()\n",
    "\n",
    "# print(stock_info_clean.count())\n",
    "\n",
    "\n",
    "# # json.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
