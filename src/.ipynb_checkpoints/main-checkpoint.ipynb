{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12fdb75-4cbe-4af2-9d8c-8da90c1400ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark_application_test, master=spark://spark-master:7077) created by getOrCreate at /tmp/ipykernel_4535/576193586.py:9 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 3\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # demo sử dụng spark đọc 1 mảng vào df\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from pyspark.context import SparkContext\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# from pyspark.sql.session import SparkSession\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# df = spark.createDataFrame(data=data, schema = columns)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# df.show()\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    427\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    435\u001b[0m             currentAppName,\n\u001b[1;32m    436\u001b[0m             currentMaster,\n\u001b[1;32m    437\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    438\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    439\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark_application_test, master=spark://spark-master:7077) created by getOrCreate at /tmp/ipykernel_4535/576193586.py:9 "
     ]
    }
   ],
   "source": [
    "# # demo sử dụng spark đọc 1 mảng vào df\n",
    "# from pyspark.context import SparkContext\n",
    "# from pyspark.sql.session import SparkSession\n",
    "\n",
    "# spark = (SparkSession\n",
    "#     .builder\n",
    "#     .master(\"spark://spark-master:7077\")\n",
    "#     .appName('pyspark_application_test')\n",
    "#     .getOrCreate())\n",
    "\n",
    "# data = [('James','','Smith','1991-04-01','M',3000),\n",
    "#   ('Michael','Rose','','2000-05-19','M',4000),\n",
    "#   ('Robert','','Williams','1978-09-05','M',4000),\n",
    "#   ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "#   ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "# ]\n",
    "\n",
    "# columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "# df = spark.createDataFrame(data=data, schema = columns)\n",
    "# df.show()\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75374fd2-2303-429c-a4b1-27530adf6286",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m     df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     df.write.format(\"json\").mode(\"overwrite\").save(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     checker = spark_instance.read.json(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     checker.printSchema()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     checker.show(1, False)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m get_stock_real_times_by_group(\u001b[43mspark\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# demo lấy api rồi đẩy vào dataframe -> ghi vào hadoop\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_stock_real_times_by_group (spark_instance):\n",
    "    print(datetime.now())\n",
    "    stock_real_times_by_group = requests.post('https://wgateway-iboard.ssi.com.vn/graphql', json = {\n",
    "        \"operationName\" : \"stockRealtimesByGroup\",\n",
    "        \"query\":  \"query stockRealtimesByGroup($group: String) {\\n  stockRealtimesByGroup(group: $group) {\\n    stockNo\\n    ceiling\\n    floor\\n    refPrice\\n    stockSymbol\\n    stockType\\n    exchange\\n    matchedPrice\\n    matchedVolume\\n    priceChange\\n    priceChangePercent\\n    highest\\n    avgPrice\\n    lowest\\n    nmTotalTradedQty\\n    best1Bid\\n    best1BidVol\\n    best2Bid\\n    best2BidVol\\n    best3Bid\\n    best3BidVol\\n    best4Bid\\n    best4BidVol\\n    best5Bid\\n    best5BidVol\\n    best6Bid\\n    best6BidVol\\n    best7Bid\\n    best7BidVol\\n    best8Bid\\n    best8BidVol\\n    best9Bid\\n    best9BidVol\\n    best10Bid\\n    best10BidVol\\n    best1Offer\\n    best1OfferVol\\n    best2Offer\\n    best2OfferVol\\n    best3Offer\\n    best3OfferVol\\n    best4Offer\\n    best4OfferVol\\n    best5Offer\\n    best5OfferVol\\n    best6Offer\\n    best6OfferVol\\n    best7Offer\\n    best7OfferVol\\n    best8Offer\\n    best8OfferVol\\n    best9Offer\\n    best9OfferVol\\n    best10Offer\\n    best10OfferVol\\n    buyForeignQtty\\n    buyForeignValue\\n    sellForeignQtty\\n    sellForeignValue\\n    caStatus\\n    tradingStatus\\n    remainForeignQtty\\n    currentBidQty\\n    currentOfferQty\\n    session\\n    tradingUnit\\n    __typename\\n  }\\n}\\n\",\n",
    "        \"variables\": {\n",
    "            \"group\": \"VN30\"\n",
    "        }\n",
    "    })\n",
    "    result = json.loads(stock_real_times_by_group.text)['data']['stockRealtimesByGroup']\n",
    "    \n",
    "    df = spark_instance.sparkContext.parallelize(result).map(lambda x: json.dumps(x))\n",
    "    df = spark_instance.read.json(df)\n",
    "    df.show()\n",
    "#     df.write.format(\"json\").mode(\"overwrite\").save(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\n",
    "#     checker = spark_instance.read.json(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\n",
    "#     checker.printSchema()\n",
    "#     checker.show(1, False)\n",
    "    \n",
    "get_stock_real_times_by_group(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e3bd07-8287-4b54-9fef-f0cdeccab534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m     df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     df.write.format(\"json\").mode(\"overwrite\").save(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     checker = spark_instance.read.json(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     checker.printSchema()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     checker.show(1, False)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m get_stock_real_times_by_group(\u001b[43mspark\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# demo lấy api rồi đẩy vào dataframe -> ghi vào hadoop\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_stock_real_times_by_group (spark_instance):\n",
    "    print(datetime.now())\n",
    "    stock_real_times_by_group = requests.post('https://wgateway-iboard.ssi.com.vn/graphql', json = {\n",
    "        \"operationName\" : \"stockRealtimesByGroup\",\n",
    "        \"query\":  \"query stockRealtimesByGroup($group: String) {\\n  stockRealtimesByGroup(group: $group) {\\n    stockNo\\n    ceiling\\n    floor\\n    refPrice\\n    stockSymbol\\n    stockType\\n    exchange\\n    matchedPrice\\n    matchedVolume\\n    priceChange\\n    priceChangePercent\\n    highest\\n    avgPrice\\n    lowest\\n    nmTotalTradedQty\\n    best1Bid\\n    best1BidVol\\n    best2Bid\\n    best2BidVol\\n    best3Bid\\n    best3BidVol\\n    best4Bid\\n    best4BidVol\\n    best5Bid\\n    best5BidVol\\n    best6Bid\\n    best6BidVol\\n    best7Bid\\n    best7BidVol\\n    best8Bid\\n    best8BidVol\\n    best9Bid\\n    best9BidVol\\n    best10Bid\\n    best10BidVol\\n    best1Offer\\n    best1OfferVol\\n    best2Offer\\n    best2OfferVol\\n    best3Offer\\n    best3OfferVol\\n    best4Offer\\n    best4OfferVol\\n    best5Offer\\n    best5OfferVol\\n    best6Offer\\n    best6OfferVol\\n    best7Offer\\n    best7OfferVol\\n    best8Offer\\n    best8OfferVol\\n    best9Offer\\n    best9OfferVol\\n    best10Offer\\n    best10OfferVol\\n    buyForeignQtty\\n    buyForeignValue\\n    sellForeignQtty\\n    sellForeignValue\\n    caStatus\\n    tradingStatus\\n    remainForeignQtty\\n    currentBidQty\\n    currentOfferQty\\n    session\\n    tradingUnit\\n    __typename\\n  }\\n}\\n\",\n",
    "        \"variables\": {\n",
    "            \"group\": \"VN30\"\n",
    "        }\n",
    "    })\n",
    "    result = json.loads(stock_real_times_by_group.text)['data']['stockRealtimesByGroup']\n",
    "    \n",
    "    df = spark_instance.sparkContext.parallelize(result).map(lambda x: json.dumps(x))\n",
    "    df = spark_instance.read.json(df)\n",
    "    df.show()\n",
    "#     df.write.format(\"json\").mode(\"overwrite\").save(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\n",
    "#     checker = spark_instance.read.json(\"hdfs://namenode:9000/project20221/raw/ssiDataApi.json\")\n",
    "#     checker.printSchema()\n",
    "#     checker.show(1, False)\n",
    "    \n",
    "get_stock_real_times_by_group(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c08c8-3af7-4b15-8c0d-dcc44cdc4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0.1411395921869795\n",
      "heloooo\n",
      "0.4669149277785203\n",
      "heloooo\n",
      "0.46891553737083835\n",
      "heloooo\n",
      "0.4521907284114207\n",
      "heloooo\n",
      "0.10809866538530098\n",
      "heloooo\n",
      "0.1545509734106829\n",
      "heloooo\n",
      "0.31363957173768675\n",
      "heloooo\n",
      "0.44689895635516574\n",
      "heloooo\n",
      "0.1782700222099973\n",
      "heloooo\n",
      "0.4342120505760544\n",
      "heloooo\n",
      "0.20609632957281943\n",
      "heloooo\n",
      "0.10275048904997437\n",
      "heloooo\n",
      "0.2996692208958267\n",
      "heloooo\n",
      "0.4069168246944229\n",
      "heloooo\n",
      "0.4807318371082262\n",
      "heloooo\n",
      "0.22143682351748112\n",
      "heloooo\n",
      "0.3059227701913425\n",
      "heloooo\n",
      "0.18240260699769975\n",
      "heloooo\n",
      "0.30889079870826885\n",
      "heloooo\n",
      "0.2241807136646149\n",
      "heloooo\n",
      "0.4478857616586951\n",
      "heloooo\n",
      "0.1592891350914777\n",
      "heloooo\n",
      "0.4780504375781629\n",
      "heloooo\n",
      "0.20181033087059885\n",
      "heloooo\n",
      "0.2343010009282186\n",
      "heloooo\n",
      "0.31803312052815197\n",
      "heloooo\n",
      "0.3928958106910687\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "print('started')\n",
    "def start_crawler ():\n",
    "    while True:\n",
    "        sleep_time = random.uniform(0.5,1.5)\n",
    "        print(sleep_time)\n",
    "        time.sleep(sleep_time)\n",
    "        print(\"heloooo\")\n",
    "        \n",
    "start_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3fd07-0239-4c94-a2df-105666d2e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "print('start')\n",
    "\n",
    "spark_sess = (\\\n",
    "    SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[1]')\\\n",
    "    .appName('pyspark_app')\\\n",
    "    .getOrCreate())\n",
    "\n",
    "print('start')\n",
    "\n",
    "csv = spark_sess.read.option(\"inferSchema\", True).option(\"header\",True).csv(\"../crawler/data/ssiDataApi.csv\")\n",
    "csv.printSchema()\n",
    "csv.show(1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd12c6-431c-49f4-a470-ec559563503d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
